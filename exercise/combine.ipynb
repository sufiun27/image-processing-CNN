{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc3b1d7b-a7b2-4c1d-9b7a-4bf749a7dc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import os\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64f57901-06e5-4632-afdd-77b331424f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense,Flatten,Dropout  \n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec058206-c279-49c8-aba1-e40f21de5228",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7  \n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa45e4d-551d-45bc-8f66-736eb4cb7964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Test', 'Train']\n",
      "Bacterial leaf blight\n",
      "images/train\\Bacterial leaf blight\\DSC_0366.jpg\n",
      "images/train\\Bacterial leaf blight\\DSC_0367.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0370.jpg\n",
      "images/train\\Bacterial leaf blight\\DSC_0372.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0373.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0375.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0376.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0377.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0378.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0380.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0381.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0382.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0384.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0386.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0388.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0389.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0390.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0393.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0396.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0397.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0398.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0399.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0400.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0401.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0402.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0403.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0405.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0406.JPG\n",
      "images/train\\Bacterial leaf blight\\DSC_0700.jpg\n",
      "images/train\\Bacterial leaf blight\\DSC_0701.jpg\n",
      "images/train\\Bacterial leaf blight\\DSC_0702.jpg\n",
      "images/train\\Bacterial leaf blight\\DSC_0703.JPG\n",
      "Brown spot\n",
      "images/train\\Brown spot\\DSC_0101.jpg\n",
      "images/train\\Brown spot\\DSC_0104.jpg\n",
      "images/train\\Brown spot\\DSC_0105.jpg\n",
      "images/train\\Brown spot\\DSC_0106.jpg\n",
      "images/train\\Brown spot\\DSC_0107.jpg\n",
      "images/train\\Brown spot\\DSC_0108.jpg\n",
      "images/train\\Brown spot\\DSC_0109.jpg\n",
      "images/train\\Brown spot\\DSC_0110.jpg\n",
      "images/train\\Brown spot\\DSC_0111.jpg\n",
      "images/train\\Brown spot\\DSC_0112.jpg\n",
      "images/train\\Brown spot\\DSC_0113.jpg\n",
      "images/train\\Brown spot\\DSC_0114.jpg\n",
      "images/train\\Brown spot\\DSC_0115.jpg\n",
      "images/train\\Brown spot\\DSC_0117.jpg\n",
      "images/train\\Brown spot\\DSC_0118.jpg\n",
      "images/train\\Brown spot\\DSC_0119.jpg\n",
      "images/train\\Brown spot\\DSC_0292.JPG\n",
      "images/train\\Brown spot\\DSC_0295.JPG\n",
      "images/train\\Brown spot\\DSC_0299.JPG\n",
      "images/train\\Brown spot\\DSC_0300.JPG\n",
      "images/train\\Brown spot\\DSC_0301.JPG\n",
      "images/train\\Brown spot\\DSC_0302.JPG\n",
      "images/train\\Brown spot\\DSC_0303.JPG\n",
      "images/train\\Brown spot\\DSC_0306.JPG\n",
      "images/train\\Brown spot\\DSC_0307.JPG\n",
      "images/train\\Brown spot\\DSC_0324.JPG\n",
      "images/train\\Brown spot\\DSC_0325.JPG\n",
      "images/train\\Brown spot\\DSC_0329.jpg\n",
      "images/train\\Brown spot\\DSC_0332.JPG\n",
      "images/train\\Brown spot\\DSC_0333.JPG\n",
      "images/train\\Brown spot\\DSC_0391.jpg\n",
      "images/train\\Brown spot\\DSC_0394.jpg\n",
      "Leaf smut\n",
      "images/train\\Leaf smut\\DSC_0308.JPG\n",
      "images/train\\Leaf smut\\DSC_0309.JPG\n",
      "images/train\\Leaf smut\\DSC_0312.JPG\n",
      "images/train\\Leaf smut\\DSC_0313.JPG\n",
      "images/train\\Leaf smut\\DSC_0314.JPG\n",
      "images/train\\Leaf smut\\DSC_0315.jpg\n",
      "images/train\\Leaf smut\\DSC_0317.JPG\n",
      "images/train\\Leaf smut\\DSC_0318.JPG\n",
      "images/train\\Leaf smut\\DSC_0319.jpg\n",
      "images/train\\Leaf smut\\DSC_0320.JPG\n",
      "images/train\\Leaf smut\\DSC_0322.jpg\n",
      "images/train\\Leaf smut\\DSC_0327.JPG\n",
      "images/train\\Leaf smut\\DSC_0328.jpg\n",
      "images/train\\Leaf smut\\DSC_0330.jpg\n",
      "images/train\\Leaf smut\\DSC_0335.JPG\n",
      "images/train\\Leaf smut\\DSC_0336.jpg\n",
      "images/train\\Leaf smut\\DSC_0338.JPG\n",
      "images/train\\Leaf smut\\DSC_0339.jpg\n",
      "images/train\\Leaf smut\\DSC_0500.jpg\n",
      "images/train\\Leaf smut\\DSC_0501.jpg\n",
      "images/train\\Leaf smut\\DSC_0502.jpg\n",
      "images/train\\Leaf smut\\DSC_0504.jpg\n",
      "images/train\\Leaf smut\\DSC_0506.jpg\n",
      "images/train\\Leaf smut\\DSC_0507.jpg\n",
      "images/train\\Leaf smut\\DSC_0508.jpg\n",
      "images/train\\Leaf smut\\DSC_0509.jpg\n",
      "images/train\\Leaf smut\\DSC_0510.jpg\n",
      "images/train\\Leaf smut\\DSC_0511.jpg\n",
      "images/train\\Leaf smut\\DSC_0512.jpg\n",
      "images/train\\Leaf smut\\DSC_0513.jpg\n",
      "images/train\\Leaf smut\\DSC_0514.jpg\n",
      "images/train\\Leaf smut\\DSC_0516.jpg\n",
      "Bacterial leaf blight\n",
      "images/test\\Bacterial leaf blight\\DSC_0365.JPG\n",
      "images/test\\Bacterial leaf blight\\DSC_0374.JPG\n",
      "images/test\\Bacterial leaf blight\\DSC_0379.JPG\n",
      "images/test\\Bacterial leaf blight\\DSC_0383.JPG\n",
      "images/test\\Bacterial leaf blight\\DSC_0385.jpg\n",
      "images/test\\Bacterial leaf blight\\DSC_0392.JPG\n",
      "images/test\\Bacterial leaf blight\\DSC_0395.JPG\n",
      "images/test\\Bacterial leaf blight\\DSC_0404.JPG\n",
      "Bacterial leaf blight.rar\n",
      "Brown spot\n",
      "images/test\\Brown spot\\DSC_0100.jpg\n",
      "images/test\\Brown spot\\DSC_0116.jpg\n",
      "images/test\\Brown spot\\DSC_0121.jpg\n",
      "images/test\\Brown spot\\DSC_0296.jpg\n",
      "images/test\\Brown spot\\DSC_0304.JPG\n",
      "images/test\\Brown spot\\DSC_0305.JPG\n",
      "images/test\\Brown spot\\DSC_0323.JPG\n",
      "images/test\\Brown spot\\DSC_0337.JPG\n",
      "Leaf smut\n",
      "images/test\\Leaf smut\\DSC_0293.JPG\n",
      "images/test\\Leaf smut\\DSC_0310.JPG\n",
      "images/test\\Leaf smut\\DSC_0316.JPG\n",
      "images/test\\Leaf smut\\DSC_0321.JPG\n",
      "images/test\\Leaf smut\\DSC_0331.JPG\n",
      "images/test\\Leaf smut\\DSC_0503.jpg\n",
      "images/test\\Leaf smut\\DSC_0505.jpg\n",
      "images/test\\Leaf smut\\DSC_0515.jpg\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"images/\"))\n",
    "\n",
    "SIZE = 224  #Resize images\n",
    "\n",
    "#Capture training data and labels into respective lists\n",
    "train_images = []\n",
    "train_labels = [] \n",
    "\n",
    "for directory_path in glob.glob(\"images/train/*\"):\n",
    "    label = directory_path.split(\"\\\\\")[-1]\n",
    "    print(label)\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
    "        print(img_path)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)       \n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        train_images.append(img)\n",
    "        train_labels.append(label)\n",
    "        \n",
    "        #Convert lists to arrays        \n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "\n",
    "# Capture test/validation data and labels into respective lists\n",
    "\n",
    "test_images = []\n",
    "test_labels = [] \n",
    "for directory_path in glob.glob(\"images/test/*\"):\n",
    "    fruit_label = directory_path.split(\"\\\\\")[-1]\n",
    "    print(fruit_label)\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.jpg\")):\n",
    "        print(img_path)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.resize(img, (SIZE, SIZE))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        test_images.append(img)\n",
    "        test_labels.append(fruit_label)\n",
    "        \n",
    "        #Convert lists to arrays                \n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "#Encode labels from text to integers.\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(test_labels)\n",
    "test_labels_encoded = le.transform(test_labels)\n",
    "le.fit(train_labels)\n",
    "train_labels_encoded = le.transform(train_labels)\n",
    "#it make the data calass name to numerical formate like 1,2,3,4 for calculation purpose\n",
    "\n",
    "#Split data into test and train datasets (already split but assigning to meaningful convention)\n",
    "x_train, y_train, x_test, y_test = train_images, train_labels_encoded, test_images, test_labels_encoded\n",
    "\n",
    "# Normalize pixel values to between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "#One hot encode y values for neural network. make tahe numamic class in a matrix\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "318b65ef-854c-4d31-8797-da3174268279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 109, 109, 96)      14208     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 54, 54, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 62,357,608\n",
      "Trainable params: 62,357,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###########################################################################################################\n",
    "#Load model wothout classifier/fully connected layers...thats means dont use the dense leyar\n",
    "model = Sequential()  \n",
    "model.add(Conv2D(96,(7,7),strides=(2,2),input_shape=(224,224,3),padding='valid',activation='relu',kernel_initializer='uniform'))  \n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model.add(Conv2D(256,(5,5),strides=(2,2),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(1000,activation='softmax'))  \n",
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])  \n",
    "model.summary()\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fadad91-7c43-4f28-9c0e-01e85f302dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 109, 109, 96)      14208     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 54, 54, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 27, 27, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4096)              37752832  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 62,357,608\n",
      "Trainable params: 0\n",
      "Non-trainable params: 62,357,608\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Make loaded layers as non-trainable. This is important as we want to work with pre-trained weights .. \n",
    "#no data will be trainable (we are not gone train this with multiple epocs  )\n",
    "\n",
    "for layer in model.layers:\n",
    "\tlayer.trainable = False\n",
    "    \n",
    "model.summary()  #Trainable parameters will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd21885-e450-4b4a-b391-e59524511aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense,Flatten,Dropout  \n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D  \n",
    "from keras.utils.np_utils import to_categorical  \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a6dbf21-5d34-4529-b149-489a8f92269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 54, 54, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 12, 12, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              26218496  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 50,844,008\n",
      "Trainable params: 50,844,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###########################################################################################################\n",
    "#Load model wothout classifier/fully connected layers...thats means dont use the dense leyar\n",
    "model1 = Sequential()  \n",
    "model1.add(Conv2D(96,(11,11),strides=(4,4),input_shape=(224,224,3),padding='valid',activation='relu',kernel_initializer='uniform'))  \n",
    "model1.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model1.add(Conv2D(256,(5,5),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model1.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model1.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model1.add(Conv2D(384,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model1.add(Conv2D(256,(3,3),strides=(1,1),padding='same',activation='relu',kernel_initializer='uniform'))  \n",
    "model1.add(MaxPooling2D(pool_size=(3,3),strides=(2,2)))  \n",
    "model1.add(Flatten())  \n",
    "model1.add(Dense(4096,activation='relu'))  \n",
    "model1.add(Dropout(0.5))  \n",
    "model1.add(Dense(4096,activation='relu'))  \n",
    "model1.add(Dropout(0.5))  \n",
    "model1.add(Dense(1000,activation='softmax'))  \n",
    "model1.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])  \n",
    "model1.summary()\n",
    "############################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "042e1c94-21e1-4184-ae88-b6cc11a46670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 54, 54, 96)        34944     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 256)       614656    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 384)       885120    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 12, 12, 384)       1327488   \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 12, 12, 256)       884992    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              26218496  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 50,844,008\n",
      "Trainable params: 0\n",
      "Non-trainable params: 50,844,008\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Make loaded layers as non-trainable. This is important as we want to work with pre-trained weights .. \n",
    "#no data will be trainable (we are not gone train this with multiple epocs  )\n",
    "\n",
    "for layer in model1.layers:\n",
    "\tlayer.trainable = False\n",
    "    \n",
    "model1.summary()  #Trainable parameters will be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5c21458-0566-45b1-be00-d1b6b11a8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "feature_extractor=model.predict(x_train)\n",
    "features = feature_extractor.reshape(feature_extractor.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02337068-635a-4d14-a50b-036aec439917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1\n",
    "feature_extractor1=model1.predict(x_train)\n",
    "features1 = feature_extractor.reshape(feature_extractor.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69b95e79-b604-4103-9e0f-64afe4b0d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_for_RF = np.concatenate((features, features1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ea697f8-636d-4639-89d1-f7db22327e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00073163 0.00074132 0.00093784 ... 0.00100307 0.00088781 0.00113183]\n",
      " [0.00072231 0.00065734 0.00096468 ... 0.00099668 0.00087783 0.00114559]\n",
      " [0.00071931 0.00078228 0.00095321 ... 0.00094807 0.00084162 0.00116955]\n",
      " ...\n",
      " [0.00071343 0.00077572 0.00111229 ... 0.00088565 0.00083252 0.00118155]\n",
      " [0.00082854 0.00088897 0.00093306 ... 0.00095004 0.00085873 0.00103908]\n",
      " [0.00085196 0.0008819  0.00107464 ... 0.00091182 0.00093631 0.00103955]]\n"
     ]
    }
   ],
   "source": [
    "print(X_for_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2f12862-fd4a-4b98-a8ee-acdc0e521638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################################################\n",
    "#RANDOM FOREST (it is classifiare problem not a regrassion problem nost use random forest regrassion) # n_estimators 50 mens it will generate 50 tree\n",
    "# you can use another classifire module there like decition tree svm etc\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "RF_model = RandomForestClassifier(n_estimators = 50, random_state = 42)\n",
    "###############################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4e29a76-d720-47b3-b076-c8aa118a93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train= np.concatenate((y_train, y_train ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30abdbe3-a94c-4a87-99d0-43c3d233c98c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=50, random_state=42)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on training data\n",
    "#For sklearn no one hot encoding x_for_rf random forest y_train (are level value)\n",
    "RF_model.fit(X_for_RF, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff450cf2-eaf6-4ef7-8e1f-0b8c820e0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_feature =model.predict(x_test)\n",
    "X_test_features = X_test_feature.reshape(X_test_feature.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a2986bb-3137-47d1-9abb-07b062df9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_feature1 =model1.predict(x_test)\n",
    "X_test_features1 = X_test_feature.reshape(X_test_feature.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b40241e-9157-44ce-8c48-dea1ff7c07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features= np.concatenate((X_test_features, X_test_features1 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2637e185-1da1-44aa-9a48-6b14469d833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now predict using the trained RF model. \n",
    "prediction_RF = RF_model.predict(X_test_features) # out put will look like 1.2.3\n",
    "\n",
    "#Inverse le transform to get original label back. \n",
    "prediction_RF = le.inverse_transform(prediction_RF) #before we make the class value like 1,2,3 numeric its re build this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f786314-381b-4401-ba18-42b9b3dbc7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels= np.concatenate((test_labels, test_labels ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b43bf238-1372-4a97-9b29-66f4e2b496da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [24, 48]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-3e43ecfc671c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction_RF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#provide test and prediction data to measure that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# Compute accuracy for each possible representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'multilabel'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[0;32m    256\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [24, 48]"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print (\"Accuracy = \", metrics.accuracy_score(test_labels, prediction_RF)) #provide test and prediction data to measure that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ef657-1960-4c7e-9885-9e833973851f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
